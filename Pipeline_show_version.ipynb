{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas datasets openai python-dotenv tqdm timeout-decorator fuzzysearch sqlalchemy regex sentence-transformers tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Callable, List, Optional\n",
    "from tqdm import tqdm\n",
    "import timeout_decorator\n",
    "import regex\n",
    "import time\n",
    "from sqlalchemy import create_engine, text\n",
    "import sqlite3\n",
    "import threading\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "from fuzzysearch import find_near_matches\n",
    "from functools import lru_cache\n",
    "import re\n",
    "import time\n",
    "from sqlalchemy import create_engine, MetaData, Table, text, types\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis_with_unique_symbols(df):\n",
    "    emoji_pattern = regex.compile(r'\\p{So}|\\p{Cn}')\n",
    "    \n",
    "    def replace_emojis(column_name):\n",
    "        # Find all emojis using regex\n",
    "        emojis = regex.findall(r'\\X', column_name)  # Match grapheme clusters\n",
    "        replacements = {}\n",
    "        for emoji in emojis:\n",
    "            # Check if it's an emoji (Unicode \"Other Symbols\" or related categories)\n",
    "            if emoji_pattern.match(emoji):\n",
    "                # Replace emoji with a unique symbol based on its Unicode representation\n",
    "                unique_symbol = f\"_IMG{'_'.join(f'{ord(c):X}' for c in emoji)}_\"\n",
    "                replacements[emoji] = unique_symbol\n",
    "        \n",
    "        # Replace all emojis in one go\n",
    "        for emoji, unique_symbol in replacements.items():\n",
    "            column_name = column_name.replace(emoji, unique_symbol)\n",
    "        return column_name\n",
    "\n",
    "    # Apply the replacement function to all column names\n",
    "    df.columns = df.columns.map(replace_emojis)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to cache preloaded datasets\n",
    "preloaded_datasets = {}\n",
    "\n",
    "def load_data(name: str = \"semeval\", split: str = \"dev\") -> Dataset:\n",
    "    \"\"\"Load dataset from local parquet file or download if not available.\"\"\"\n",
    "    local_path = f\"./{name}_{split}.parquet\"\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        return Dataset.from_parquet(local_path)\n",
    "    \n",
    "    dataset = load_dataset(\"cardiffnlp/databench\", name=name, split=split)\n",
    "    dataset.to_parquet(local_path)\n",
    "    return dataset\n",
    "\n",
    "def load_table(dataset_name: str, is_sample: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Load table from local parquet file or remote source if not cached.\"\"\"\n",
    "    file_type = \"sample\" if is_sample else \"all\"\n",
    "    key = f\"{dataset_name}_{file_type}\"\n",
    "    local_path = f\"./{dataset_name}_{file_type}.parquet\"\n",
    "    \n",
    "    if key not in preloaded_datasets:\n",
    "        if os.path.exists(local_path):\n",
    "            df = pd.read_parquet(local_path)\n",
    "        else:\n",
    "            df = pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{dataset_name}/{file_type}.parquet\")\n",
    "            \n",
    "        df.columns = df.columns.str.replace(r'<gx:.*?>', '', regex=True)\n",
    "        df = replace_emojis_with_unique_symbols(df)\n",
    "        # df.columns = [f\"{col}_{i}\" for i, col in enumerate(df.columns)]\n",
    "        df.columns = [f\"{col}\" for col in df.columns]\n",
    "        df._processed = True\n",
    "        preloaded_datasets[key] = df\n",
    "    else:\n",
    "        df = preloaded_datasets[key]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def load_sample(name: str = \"qa\") -> pd.DataFrame:\n",
    "    \"\"\"Load sample data from local parquet file or remote source if not available.\"\"\"\n",
    "    local_path = f\"./{name}_sample.parquet\"\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        return pd.read_parquet(local_path)\n",
    "    \n",
    "    return pd.read_parquet(f\"hf://datasets/cardiffnlp/databench/data/{name}/sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIClient:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.client = openai.OpenAI(\n",
    "            api_key=os.getenv('OPENROUTER_API_KEY2'),\n",
    "            base_url=\"https://openrouter.ai/api/v1\"\n",
    "        )\n",
    "\n",
    "    def generate_response(self, prompt: str, max_tokens=3000, retries=3, model=\"meta-llama/llama-3.3-70b-instruct\") -> str:\n",
    "        sleep_durations = [15, 30, 60]  # Sleep durations in seconds\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a best in class instruction following assistant. You excel in tasks with data. You reason very thorougly and step-by-step.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0,\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "                # print(response)\n",
    "\n",
    "                if response.choices:\n",
    "                    content = response.choices[0].message.content.strip()\n",
    "                    # print(f\"{model}: {content}\")\n",
    "                    return content\n",
    "                else:\n",
    "                    raise ValueError(\"No response choices available.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed with error: {str(e)}\")\n",
    "                if attempt < len(sleep_durations):\n",
    "                    print(f\"Sleeping for {sleep_durations[attempt]} seconds before retrying...\")\n",
    "                    time.sleep(sleep_durations[attempt])\n",
    "                else:\n",
    "                    return f\"__CODE_GEN_ERROR__: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_columns_only(df, question, ai_client):\n",
    "     #Get relevant columns\n",
    "    prompt = f\"Given the question: '{question}', which columns from the dataset are necessary to answer it? The dataset contains the following columns: {', '.join(df.columns)}. Please provide extensive reasoning and only then the column names as a comma-separated list started with [ and ended with ].\"\n",
    "    # print(prompt)\n",
    "    columns_text = ai_client.generate_response(prompt, model=\"meta-llama/llama-3.2-3b-instruct\").strip()\n",
    "    # print(f\"Columns identified: {columns_text}\")\n",
    "\n",
    "    columns_list = [col.strip().strip(\"'\\\"`\") for col in columns_text[columns_text.rfind('[')+1:columns_text.rfind(']')].split(',')]\n",
    "    # print(f\"Columns identified2: {columns_list}\")\n",
    "\n",
    "    return df[columns_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Initialize model globally to avoid reloading\n",
    "_model = None\n",
    "\n",
    "def get_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    return _model\n",
    "\n",
    "def get_relevant_rows_by_cosine_similarity(df, question, ai_client):\n",
    "    # Use GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = get_model().to(device)\n",
    "\n",
    "    # Get relevant columns efficiently without copy\n",
    "    df = get_relevant_columns_only(df, question, ai_client=ai_client)\n",
    "    \n",
    "    # Vectorized string concatenation\n",
    "    text_data = df.astype(str).values.sum(axis=1)\n",
    "    \n",
    "    try:\n",
    "        # Sequential encoding\n",
    "        embeddings = model.encode(\n",
    "            text_data,\n",
    "            batch_size=256,\n",
    "            show_progress_bar=False,\n",
    "            convert_to_tensor=True,\n",
    "            device=device\n",
    "        )\n",
    "        question_embedding = model.encode(\n",
    "            [question],\n",
    "            show_progress_bar=False,\n",
    "            convert_to_tensor=True,\n",
    "            device=device\n",
    "        )[0]\n",
    "\n",
    "        # Vectorized similarity calculation\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        question_embedding = question_embedding.cpu().numpy()\n",
    "        \n",
    "        # Fast cosine similarity using matrix operations\n",
    "        similarities = np.dot(embeddings, question_embedding)\n",
    "        norms = np.linalg.norm(embeddings, axis=1) * np.linalg.norm(question_embedding)\n",
    "        similarities = np.divide(similarities, norms, where=norms!=0)\n",
    "        \n",
    "        # Fast filtering using numpy operations\n",
    "        top_indices = np.argpartition(similarities, -10)[-10:]\n",
    "        return df.iloc[top_indices]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in similarity calculation: {str(e)}\")\n",
    "        return df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(responses: List[str], dataset, is_sample: bool = True) -> Tuple[float, List]:\n",
    "    start_time = time.time()\n",
    "    correct = 0\n",
    "    truths = dataset[\"sample_answer\" if is_sample else \"answer\"]\n",
    "    logs = []\n",
    "    \n",
    "    if not responses or not truths:\n",
    "        print(\"Responses or truths are empty, skipping evaluation.\")\n",
    "        return (0.0, logs)\n",
    "    \n",
    "    for response, truth, semantic, question in zip(responses, truths, dataset[\"type\"], dataset[\"question\"]):\n",
    "        is_correct = compare_results(response, truth, semantic)\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(response, \" || \" , truth, \"||\", question, f\"verdict={is_correct}\" )\n",
    "        logs.append((response, truth, semantic, question, is_correct))\n",
    "    \n",
    "    if not logs:\n",
    "        print(\"No logs were generated during evaluation.\")\n",
    "    \n",
    "    return (correct / len(truths), logs)\n",
    "\n",
    "def compare_results(value, truth, semantic):\n",
    "    STRIP_CHARS = \"[]'\\\" \"\n",
    "    semantic = semantic.strip()\n",
    "    result = False\n",
    "    \n",
    "    if semantic == \"boolean\":\n",
    "        result = str(value).strip(STRIP_CHARS).lower() == str(truth).strip(STRIP_CHARS).lower()\n",
    "    elif semantic == \"category\":\n",
    "        if value is None and truth is None:\n",
    "            result = True\n",
    "        elif value is None or truth is None:\n",
    "            result = False\n",
    "        else:\n",
    "            value_str = str(value).strip(STRIP_CHARS)\n",
    "            truth_str = str(truth).strip(STRIP_CHARS)\n",
    "            if value_str == truth_str:\n",
    "                result = True\n",
    "            else:\n",
    "                try:\n",
    "                    value_date = pd.to_datetime(value_str).date()\n",
    "                    truth_date = pd.to_datetime(truth_str).date()\n",
    "                    result = value_date == truth_date\n",
    "                except (ValueError, TypeError):\n",
    "                    if not value_str and not truth_str:\n",
    "                        result = True\n",
    "                    else:\n",
    "                        result = value_str == truth_str\n",
    "    elif semantic == \"number\":\n",
    "        try:\n",
    "            value_cleaned = ''.join(char for char in str(value) if char.isdigit() or char in ['.', '-'])\n",
    "            truth_cleaned = ''.join(char for char in str(truth) if char.isdigit() or char in ['.', '-'])\n",
    "            result = round(float(value_cleaned), 2) == round(float(truth_cleaned), 2)\n",
    "        except:\n",
    "            result = False\n",
    "    elif semantic == \"list[category]\":\n",
    "        try:\n",
    "            value_list = [item.strip(STRIP_CHARS) for item in str(value).strip('[]').split(',')]\n",
    "            truth_list = [item.strip(STRIP_CHARS) for item in str(truth).strip('[]').split(',')]\n",
    "            if len(value_list) != len(truth_list):\n",
    "                result = False\n",
    "            else:\n",
    "                try:\n",
    "                    value_dates = [pd.to_datetime(item).date() for item in value_list]\n",
    "                    truth_dates = [pd.to_datetime(item).date() for item in truth_list]\n",
    "                    result = set(value_dates) == set(truth_dates)\n",
    "                except (ValueError, TypeError):\n",
    "                    result = set(value_list) == set(truth_list)\n",
    "        except Exception as exc:\n",
    "            result = False\n",
    "    elif semantic == \"list[number]\":\n",
    "        try:\n",
    "            value_list = sorted(round(float(''.join(c for c in v.strip() if c.isdigit() or c in ['.', '-'])), 2) for v in str(value).strip('[]').split(',') if v.strip())\n",
    "            truth_list = sorted(round(float(''.join(c for c in t.strip() if c.isdigit() or c in ['.', '-'])), 2) for t in str(truth).strip('[]').split(',') if t.strip())\n",
    "            \n",
    "            if len(value_list) != len(truth_list):\n",
    "                result = False\n",
    "            else:\n",
    "                result = set(value_list) == set(truth_list)\n",
    "        except Exception as exc:\n",
    "            result = False\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported semantic type: {semantic}. Please use one of the following: 'text', 'number', 'list[category]', 'list[number]'.\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL/Pandas Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_python_execution(code: str, dataset_name: str, is_sample: bool, timeout: int = 5) -> Tuple[str, bool]:\n",
    "    def execute_with_timeout(code, local_vars, timeout):\n",
    "        exception = [None]\n",
    "        \n",
    "        def target():\n",
    "            try:\n",
    "                # Ensure pandas and numpy are available in the execution context\n",
    "                exec(f\"import numpy as np; import pandas as pd; {code}\", {}, local_vars)\n",
    "                local_vars['success'] = True\n",
    "            except Exception as e:\n",
    "                local_vars['result'] = str(e)\n",
    "                local_vars['success'] = False\n",
    "\n",
    "        thread = threading.Thread(target=target)\n",
    "        thread.start()\n",
    "        thread.join(timeout)\n",
    "        if thread.is_alive():\n",
    "            return \"Execution timed out\", False\n",
    "        if exception[0]:\n",
    "            return str(exception[0]), False\n",
    "        return local_vars.get('result', \"Unknown error\"), local_vars.get('success', False)\n",
    "\n",
    "    # Load the dataset and prepare local variables\n",
    "    df = load_table(dataset_name, is_sample)\n",
    "    local_vars = {'df': df, 'pd': pd, 'np': np}\n",
    "    \n",
    "    # Execute the code with a timeout\n",
    "    return execute_with_timeout(code, local_vars, timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(row: dict, is_sample) -> str:\n",
    "    \"\"\"\n",
    "    Generate prompt for the LLM.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    dataset = row[\"dataset\"]\n",
    "    question = row[\"question\"]\n",
    "    df = load_table(dataset, is_sample)\n",
    "\n",
    "    exp_prompt2 = f\"\"\"\"\n",
    "    \n",
    "1. You are two of the most esteemed Pandas DataScientists engaged in a heated and truth-seeking debate. You are presented with a dataframe and a question. Begin dialogue by rigorously discussing your reasoning step by step, ensuring to address all aspects of the checklist. In your discourse, meticulously articulate the variable type necessary to derive the answer and confirm that each column referenced is indeed present in the dataframe. Conclude your debate by providing the code to answer the question, ensuring that the variable result is explicitly assigned to the answer. Remember, all code must be presented in a single line, with statements separated by semicolons. \n",
    "2. Refrain from importing any additional libraries beyond pandas and numpy.\n",
    "3. The dataframe, df, is already populated with data for your analysis; do not initialize it, but focus solely on manipulating df to arrive at the answer.\n",
    "4. If the question requires multiple entries, always utilize .tolist() to present the results.\n",
    "5. If the question seeks a single entry, ensure that only one value is output, even if multiple entries meet the criteria.\n",
    "\n",
    "You MUST FOLLOW THE CHECKLIST, ANSWER EACH OF ITS QUESTIONS (REASONING STEP), AND ONLY THEN OUTPUT THE FINAL ANSWER BASED ON THOSE ANSWERS:\n",
    "1) How many values should be in the output?\n",
    "2) Values (or one value) from which column (only one!) should the answer consist of?\n",
    "3) What should be the type of value in the answer? \n",
    "\n",
    "Example of a task:\n",
    "Question: Identify the top 3 departments with the most employees. \n",
    "<Columns> = ['department', 'employee_id'] \n",
    "<First_row> = ('department': 'HR', 'employee_id': 101)\n",
    "Reasoning: Count the number of employees in each department, sort, and get the top 3. The result should be a list of department names. \n",
    "Checklist: \n",
    "1) The output should consist of 3 values.\n",
    "2) The values should come from the 'department' column.\n",
    "3) The type of value in the answer should be a list of strings.\n",
    "Code: result = df['department'].value_counts().nlargest(3).index.tolist()\n",
    "\n",
    "Your data to process:\n",
    "<question> = {question}\n",
    "\n",
    "- Make absolute sure that all columns used in query are present in the table.\n",
    "<columns_in_the_table> = {[col for col in df.columns]}\n",
    "<first_rows_of_table> = {df.head(3).to_string()}\n",
    "YOUR Reasoning through dialogue and Code (Start final code part by \"Code:\"):    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    experimental_prompt = f\"\"\" \n",
    "1. You are a best in the field Pandas DataScientist. You are given a dataframe and a question. You should spell out your reasoning step by step and only then provide code to answer the question. In the reasoning state it is essentianl to spell out the answers' variable type that should be sufficient to answer the question. Also spell out that each column used is indeed presented in the table. In the end of your code the variable result must be assigned to the answer to the question. One trick: all code should be in one line separated by ; (semi-columns) but it is no problem for you. \n",
    "2. Avoid importing any additional libraries than pandas and numpy.\n",
    "3. All data is already loaded into df dataframe for you, you MUST NOT initialise it, rather present only manipulations on df to calculate the answer. \n",
    "4. If the question ask for several entries alsways use .tolist().\n",
    "5. If the question ask for one entry, make sure to output only one, even if multiple qualify.\n",
    "\n",
    "You MUST FOLLOW THE CHECKLIST, ANSWER EACH OF ITS QUESTION (REASONING STEP) AND ONLY THEN OUTPUT THE FINAL ANSWER BASED ON THOSE ANSWERS:\n",
    "1) How many values should be in the output?\n",
    "2) Values (or one value) from which column (only one!) should the answer consist of?\n",
    "3) What is should be the type of value in the answer? \n",
    "\n",
    "Example:\n",
    "Question: What are the top 5 cities with the highest average temperature? \n",
    "<Columns> = ['city', 'temperature'] \n",
    "<First_row> = ('city': 'New York', 'temperature': 85)\n",
    "Reasoning: Calculate the average temperature for each city, sort them, and select the top 5. The result should be a list of city names. \n",
    "Checklist: \n",
    "1) The output should consist of 5 values.\n",
    "2) The values should come from the 'city' column.\n",
    "3) The type of value in the answer should be a list of strings.\n",
    "Code: result = df.groupby('city')['temperature'].mean().nlargest(5).index.tolist()\n",
    "\n",
    "Example:\n",
    "Question: Which product has the highest sales volume? \n",
    "<Columns> = ['product', 'sales'] \n",
    "<First_row> = ('product': 'Laptop', 'sales': 150)\n",
    "Reasoning: Sum the sales volume for each product and find the one with the maximum total. The result should be a single product name as a string. \n",
    "Checklist: \n",
    "1) The output should consist of 1 value.\n",
    "2) The value should come from the 'product' column.\n",
    "3) The type of value in the answer should be a string.\n",
    "Code: sales_volume = df.groupby('product')['sales'].sum(); result = sales_volume.idxmax()\n",
    "\n",
    "Example:\n",
    "Question: Identify the top 3 departments with the most employees. \n",
    "<Columns> = ['department', 'employee_id'] \n",
    "<First_row> = ('department': 'HR', 'employee_id': 101)\n",
    "Reasoning: Count the number of employees in each department, sort, and get the top 3. The result should be a list of department names. \n",
    "Checklist: \n",
    "1) The output should consist of 3 values.\n",
    "2) The values should come from the 'department' column.\n",
    "3) The type of value in the answer should be a list of strings.\n",
    "Code: result = df['department'].value_counts().nlargest(3).index.tolist()\n",
    "\n",
    "Your data to process:\n",
    "<question> = {question}\n",
    "\n",
    "- Make absolute sure that all columns used in query are present in the table.\n",
    "<columns_in_the_table> = {[col for col in df.columns]}\n",
    "<first_rows_of_table> = {df.head(3).to_markdown()}\n",
    "YOUR Reasoning and Code (Start code part by \"Code:\"):\n",
    "\"\"\"\n",
    "    # print([col for col in df.columns])\n",
    "    return exp_prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reflection_prompt(question: str, df: pd.DataFrame, tracebacks: List[str]) -> str:\n",
    "    return (\n",
    "        \"The following solutions failed for the task: \\\"{question}\\\"\\n\\n\"\n",
    "        + '\\n'.join([f'Solution {i+1} Error:\\n{traceback}\\n' for i, traceback in enumerate(tracebacks)])\n",
    "        + \"\\nDF info: \\n\"\n",
    "        + \"<columns_to_use> = \" + str([(col, str(df[col].dtype)) for col in df.columns]) + \"\\n\"\n",
    "        + \"<first_row_of_table> = \" + str(df.head(1).to_dict(orient='records')[0]) + \"\\n\"\n",
    "        + \"YOUR answer in a single line of pandas code:\\n\"\n",
    "        + \"Please craft a new solution considering these tracebacks. Output only fixed solution in one line:\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_voting_prompt(solutions: List[Dict], question: str, dataset_name: str, is_sample: bool):\n",
    "    df = load_table(dataset_name, is_sample)\n",
    "\n",
    "    voting_prompt = f\"\"\"\n",
    "Examples of deducing answer types:  \n",
    "1. If the question is \"Do we have respondents who have shifted their voting preference?\" the answer type is **Boolean** because the response should be True/False.  \n",
    "2. If the question is \"How many respondents participated in the survey?\" the answer type is **Integer**\n",
    "3. If the question is \"List the respondents who preferred candidate X?\" the answer type is **List** because the response requires a collection of values.  \n",
    "4. If the question is \"What is the average age of respondents?\" the answer type is **Number** because the response should be a decimal value.  \n",
    "5. If the question is \"What is the name of the candidate with the highest votes?\" the answer type is **String** because the response is a single textual value.  \n",
    "\n",
    "Given the following solutions and their results for the task: \"{question}\"  \n",
    "\n",
    "{'\\n'.join([f'Solution Number {i+1}:\\nCode: {r[\"code\"]} Answer: {str(r[\"result\"])[:50]} (may be truncated)\\n' for i, r in enumerate(solutions)])}  \n",
    "\n",
    "Instructions:  \n",
    "- Deduce the most probable and logical result to answer the given question. Then output the number of the chosen answer.\n",
    "- If you are presented with end-to-end solution, it should not be trusted for numerical questions, but it is okay for other questions.\n",
    "- Make absolute sure that all columns used in solutions are present in the table. SQL query may use additional double quotes around column names, it's okay, always put them. Real Tables columns are: {df.columns}\n",
    "- If the column name contain emoji or unicode character make sure to also include it in the column names in the query.\n",
    "- If several solutions are correct, return the lowest number of the correct solution.  \n",
    "- Otherwise, return the solution number that is most likely correct.\n",
    "- If the question ask for one entry, make sure to output only one, even if multiple qualify.\n",
    "\n",
    "You should spell out your reasoning step by step and only then provide code to answer the question. In the reasoning state it is essentianl to spell out the answers' variable type that should be sufficient to answer the question. Also spell out that each column used is indeed presented in the table. The most important part in your reasoning should be dedicated to comparing answers(results) from models and deducing which result is the most likely to be correct, then choose the model having this answer.\n",
    "First, predict the answer type for the question. Then give your answer which is just number of correct answer with predicted variable type.  Start reasoning part with \"REASONING:\" and final answer with \"ANSWER:\".\n",
    "\"\"\"\n",
    "    return voting_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_prompt(failed_solutions, question, column_names, df, ai_client):\n",
    "        return f\"\"\"\n",
    "Some Python attempts failed with errors:\n",
    "{', '.join([r[\"result\"] for r in failed_solutions])}\n",
    "\n",
    "The task was: {question}\n",
    "\n",
    "Here are some examples of SQL queries for similar tasks:\n",
    "Example 1:\n",
    "Task: Is there any entry where age is greater than 30?\n",
    "REASONING: \n",
    "1. Identify the column of interest, which is 'age'.\n",
    "2. Determine the condition to check, which is 'age > 30'.\n",
    "3. Use the SELECT statement to retrieve a boolean result indicating the presence of such entries.\n",
    "4. Apply the WHERE clause to filter rows based on the condition 'age > 30'.\n",
    "5. Use the EXISTS clause to ensure the query outputs 'True' if any row matches the condition, otherwise 'False'.\n",
    "6. Ensure the table name is 'temp_table' and the column name is enclosed in double quotes to handle any spaces or special characters.\n",
    "7. Verify that the query outputs 'True' or 'False' when presented with a yes or no question.\n",
    "CODE: ```SELECT CASE WHEN EXISTS(SELECT 1 FROM temp_table WHERE \"age\" > 30) THEN 'True' ELSE 'False' END;```\n",
    "\n",
    "Example 2:\n",
    "Task: Count the number of entries with a salary above 50000.\n",
    "REASONING: \n",
    "1. Identify the column of interest, which is 'salary'.\n",
    "2. Determine the condition to filter the data, which is 'salary > 50000'.\n",
    "3. Use the SELECT COUNT(*) statement to count the number of rows that meet the condition.\n",
    "4. Apply the WHERE clause to filter rows based on the condition 'salary > 50000'.\n",
    "5. Ensure the table name is 'temp_table' and the column name is enclosed in double quotes to handle any spaces or special characters.\n",
    "CODE: ```SELECT COUNT(*) FROM temp_table WHERE [salary] > 50000;```\n",
    "\n",
    "Write a correct fault-proof SQL SELECT query that solves this precise task.\n",
    "Rules:\n",
    "- Your SQL query should be simple with just SELECT statement, without WITH clauses.    \n",
    "- Your SQL query should output the answer, without a need to make any intermediate calculations after its finish\n",
    "- Use only basic SQL operations from SQLAlchemy (SELECT, FROM, WHERE, GROUP BY, etc.)\n",
    "- Make sure not to use \"TOP\" operation as it is not presented in SQLite \n",
    "- If present with YES or NO question, Query MUST return 'True' or 'False'\n",
    "- Write pure SQL only without any quotes around the query\n",
    "- If the question asks about several values, your query should return a list\n",
    "- If the question ask for one entry, make sure to output only one, even if multiple qualify.\n",
    "- Equip each column name into double quotes\n",
    "- Equip each string literal into double quotes\n",
    "- Use COALESCE( ..., 0) to answer with 0 if no rows are found and the question asks for the number of something.\n",
    "- If it is Yes/No question, make sure that your query output only True or False.\n",
    "- In the reasoning spell out that each column used is indeed presented in the table.\n",
    "- Enclose your code into ```\n",
    "- SELECT close MUST contain ONLY ONE column. For example, it must be only author's name, not name and id.\n",
    "- Before writing code give extensive yet precise and specific reasoning for each step of your solution. Start reasoning part by \"REASONING:\" and code part by \"CODE:\"\n",
    "\n",
    "Table name is 'temp_table'.\n",
    "Available columns and types: {', '.join([f\"{col}: {str(type(df[col].iloc[0]))}\" for col in column_names])}\n",
    "\n",
    "\n",
    "Top 3 rows with highest cosine similarity: {get_relevant_rows_by_cosine_similarity(df, question, ai_client).head(3).to_markdown()}\n",
    "YOUR RESPONSE:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_python_solutions(question: str, dataset_name: str, is_sample: bool, ai_client, n_attempts: int = 1) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get multiple solutions from LLM and execute them.\n",
    "    \"\"\"\n",
    "    solutions = []\n",
    "\n",
    "    for i in range(n_attempts):\n",
    "        model = select_model(i)\n",
    "\n",
    "        start_time_prompt = time.time()\n",
    "        prompt = generate_prompt({\"question\": question, \"dataset\": dataset_name}, is_sample)\n",
    "        end_time_prompt = time.time()\n",
    "        print(f\"Time for generate_prompt: {end_time_prompt - start_time_prompt:.4f} seconds\")\n",
    "\n",
    "        start_time_response = time.time()\n",
    "        code_response = ai_client.generate_response(prompt, model=model)\n",
    "        # print(code_response)\n",
    "        end_time_response = time.time()\n",
    "        print(f\"Time for generate_response: {end_time_response - start_time_response:.4f} seconds\")\n",
    "\n",
    "        start_time_clean = time.time()\n",
    "        code_response = clean_code_response(code_response)\n",
    "        end_time_clean = time.time()\n",
    "        print(f\"Time for clean_code_response: {end_time_clean - start_time_clean:.4f} seconds\")\n",
    "\n",
    "        try:\n",
    "            start_time_execution = time.time()\n",
    "            result, success = try_python_execution(code_response, dataset_name, is_sample)\n",
    "            end_time_execution = time.time()\n",
    "            print(f\"Time for try_python_execution: {end_time_execution - start_time_execution:.4f} seconds\")\n",
    "        except Exception as e:\n",
    "            result, success = None, False\n",
    "            print(f\"An error occurred during execution: {e}\")\n",
    "\n",
    "        result = format_result(result)\n",
    "        solutions.append({\"code\": code_response, \"result\": result, \"success\": success})\n",
    "\n",
    "    # log_solutions(question, dataset_name, solutions[-1])\n",
    "\n",
    "    if sum(solution['success'] for solution in solutions) < 2:\n",
    "        solutions.extend(handle_failed_solutions(question, dataset_name, is_sample, ai_client, solutions, n_attempts))\n",
    "\n",
    "    return solutions\n",
    "\n",
    "def select_model(attempt: int) -> str:\n",
    "    models = [\"mistralai/codestral-2501\", \"meta-llama/llama-3.3-70b-instruct\", \"qwen/qwen-2.5-coder-32b-instruct\"]\n",
    "    return models[attempt % len(models)]\n",
    "\n",
    "def clean_code_response(code_response: str) -> str:\n",
    "    \n",
    "    matches = find_near_matches('Code:', code_response, max_l_dist=2)\n",
    "    if matches:\n",
    "        code_response = code_response[matches[-1].end:]\n",
    "    else:\n",
    "        raise ValueError(\"Expected 'Code:' in the response but not found.\")\n",
    "    \n",
    "    # code_response = code_response.split(\"```\", 1)[-1] if \"```\" in code_response else code_response\n",
    "    code_response = code_response.replace(\"*\", \"\").replace(\"`\", \"\").replace(\"python\", \"\").replace('return', '').strip()\n",
    "    code_response = code_response.splitlines()[-1] if code_response.splitlines() else \"\"\n",
    "    import re\n",
    "\n",
    "    # Remove specific DataFrame creation pattern from code_response\n",
    "    code_response = re.sub(r\"df\\s*=\\s*pd\\.DataFrame\\(\\[\\{.*?\\}\\]\\);\", \"\", code_response, flags=re.DOTALL)\n",
    "    return code_response\n",
    "\n",
    "def format_result(result) -> str:\n",
    "    if isinstance(result, str):\n",
    "        result = result.replace('`', '').replace('\"', '').replace(\"'\", '')\n",
    "    elif isinstance(result, list):\n",
    "        result = str(result[0]) if len(result) == 1 else str(result)\n",
    "    return result\n",
    "\n",
    "def log_solutions(question: str, dataset_name: str, solutions: List[Dict]):\n",
    "    with open(\"1_test.txt\", \"a\", encoding='utf-8') as f:\n",
    "        f.write(f\"---\\n{question}\\n{load_table(dataset_name).columns}\\n{solutions['code']}\\n{solutions['result']}\\n----\")\n",
    "\n",
    "def handle_failed_solutions(question: str, dataset_name: str, is_sample: bool, ai_client, solutions: List[Dict], n_attempts: int) -> List[Dict]:\n",
    "\n",
    "    df = load_table(dataset_name, is_sample)\n",
    "    tracebacks = [solution['result'] for solution in solutions if isinstance(solution['result'], str)]\n",
    "    reflection_prompt = create_reflection_prompt(question, df, tracebacks)\n",
    "    new_solutions = []\n",
    "    for j in range(n_attempts):\n",
    "        model = select_model(j)\n",
    "        new_code_response = ai_client.generate_response(reflection_prompt, model=model)\n",
    "        new_code_response = clean_code_response(new_code_response)\n",
    "        result, success = try_python_execution(new_code_response, dataset_name, is_sample)\n",
    "        new_solutions.append({\"code\": new_code_response, \"result\": result, \"success\": success})\n",
    "    return new_solutions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL code gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, types, event, text\n",
    "from functools import lru_cache\n",
    "from sqlalchemy.types import UnicodeText\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Cache single SQLite in-memory connection\"\"\"\n",
    "    engine = create_engine('sqlite:///:memory:', \n",
    "                         echo=False\n",
    "                         )\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_fallback(dataset_name: str, question: str, is_sample: bool, failed_solutions: List[Dict], ai_client, n_tries: int = 1) -> List[Dict]:\n",
    "\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "    engine = get_db_connection()\n",
    "    # print(f\"get_db_connection took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    table_name = f\"table_{dataset_name}_{(str(is_sample))}\"  # Unique table name\n",
    "    # print(f\"Table name generation took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    df = load_table(dataset_name, is_sample)\n",
    "    # print(f\"load_table took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists='replace', dtype={'column_name': types.UnicodeText})\n",
    "    # print(f\"df.to_sql took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=engine)\n",
    "    # print(f\"Metadata reflection took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    temp_table = Table(table_name, metadata, autoload_with=engine)\n",
    "    column_names = [column.name for column in temp_table.columns]\n",
    "    # print(f\"Table loading and column extraction took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    prompt = get_sql_prompt(failed_solutions, question, column_names, df, ai_client).replace(\"temp_table\", table_name)\n",
    "    # print(prompt)\n",
    "    # print(f\"get_sql_prompt took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "    solutions = []\n",
    "    errors = []\n",
    "\n",
    "    for attempt in range(n_tries):\n",
    "        import time\n",
    "        model = select_model(attempt)\n",
    "        start_time = time.time()\n",
    "        if attempt == 0:\n",
    "            sql_response = ai_client.generate_response(prompt, model=model)\n",
    "        else:\n",
    "            error_msg = \"; \".join(errors)\n",
    "            sql_response = ai_client.generate_response(f\"Previous solution failed with error: {error_msg}. Write a correct fault-proof SQL SELECT query that solves this precise task. The task was: {question}\\n\" + prompt)\n",
    "\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        matches = find_near_matches('CODE:', sql_response, max_l_dist=2)\n",
    "        if not matches:\n",
    "            raise ValueError(\"Expected 'CODE:' in the response but not found.\")\n",
    "        sql_response = sql_response[matches[0].end:sql_response.rfind(\"`\")].replace(\"sql\", \"\").replace(\"`\", \"\").strip()\n",
    "        \n",
    "        with open(\"sql_few_shot_logs_a.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(f\"question={question}\\n\")\n",
    "            f.write(\"Column names: \"+ \" \".join(column_names) + \"\\n\")\n",
    "            f.write(sql_response + \"\\n\")\n",
    "        \n",
    "        # print(f\"SQL response processing took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            with engine.connect() as connection:\n",
    "                result = connection.execute(text(sql_response))\n",
    "                rows = result.fetchall()\n",
    "            # print(f\"SQL execution and fetching took {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "            if len(rows) > 1:\n",
    "                solutions.append({\n",
    "                    \"code\": sql_response,\n",
    "                    \"result\": str([row[0] for row in rows]).replace('\"', '').replace('`', '').replace(\"sql\", \"\"),\n",
    "                    \"success\": True\n",
    "                })\n",
    "            elif rows:\n",
    "                solutions.append({\n",
    "                    \"code\": sql_response,\n",
    "                    \"result\": str(rows[0][0]),\n",
    "                    \"success\": True\n",
    "                })\n",
    "            else:\n",
    "                solutions.append({\n",
    "                    \"code\": sql_response,\n",
    "                    \"result\": \"__NO_RESULT__\",\n",
    "                    \"success\": False\n",
    "                })\n",
    "        except Exception as e:\n",
    "            errors.append(str(e))\n",
    "            # print(str(e))\n",
    "            solutions.append({\n",
    "                \"code\": sql_response,\n",
    "                \"result\": \"__EXECUTION_FAILED__\",\n",
    "                \"success\": False\n",
    "            })\n",
    "   \n",
    "    return solutions if any(sol[\"success\"] for sol in solutions) else []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_on_solutions(solutions: List[Dict], question: str, dataset_name: str, is_sample, ai_client) -> str:\n",
    "    \"\"\"\n",
    "    Use LLM to vote on the best solution among successful attempts.\n",
    "    \"\"\"\n",
    "    if len(solutions) == 1:\n",
    "        return solutions[0][\"result\"]\n",
    "\n",
    "    # Generate voting prompt and get response from AI client\n",
    "    voting_prompt = generate_voting_prompt(solutions, question, dataset_name, is_sample)\n",
    "    response = ai_client.generate_response(voting_prompt, model=\"meta-llama/llama-3.3-70b-instruct\").replace(\"return\", \"\").strip()\n",
    "\n",
    "    # Log the response to a file for debugging\n",
    "    with open(\"voting_log.txt\", \"a\", encoding='utf-8') as f:\n",
    "\n",
    "        trimmed_voting_prompt = f'Given the following solutions and their results for the task: \"{question}\"\\n\\n' + \\\n",
    "            '\\n'.join([f'Solution Number {i+1}:\\nCode: {r[\"code\"]} Answer: {str(r[\"result\"])[:50]} (may be truncated)\\n' for i, r in enumerate(solutions)])\n",
    "        \n",
    "        f.write(f\"-------------\\nVoting Prompt:\\n{trimmed_voting_prompt}\\n\\nResponse:\\n{response}\\n-------------------------\")\n",
    "\n",
    "    # Extract the solution number from the response\n",
    "    response = response.split(\"ANSWER:\")[-1].strip()\n",
    "    solution_number = None\n",
    "    # Use regex to find the solution number in the response\n",
    "    match = re.search(r'\\b\\d+\\b', response)\n",
    "    if match:\n",
    "        solution_number = int(match.group())\n",
    "    # Return the result of the selected solution if valid, otherwise return the first solution\n",
    "    if solution_number is not None and 1 <= solution_number <= len(solutions):\n",
    "        return solutions[solution_number - 1][\"result\"]\n",
    "    return solutions[0][\"result\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E2E solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e_response(question, dataset_text, ai_client) -> List:\n",
    "    prompt = f'''\n",
    "    Question: {question}\n",
    "    Dataset: {dataset_text}\n",
    "\n",
    "    Analyze the data. Provide your final answer to the question based on the data. \n",
    "    If the question assumes several answers, use a list. Your answer should be in the form of one of the following:\n",
    "    1. Boolean (True/False)\n",
    "    2. List (e.g., ['Tree', 'Stone'])\n",
    "    3. Number (e.g., 5)\n",
    "    4. String (e.g., 'Spanish')\n",
    "\n",
    "    Give extensive reasoning and then fianlly provide the answer starting with string \"Final Answer:\" in one of the four formats presented above (Boolean, List, Number, String). Your response should then be finished.\n",
    "    \n",
    "    '''\n",
    "    # with open(\"prompt_log.txt\", \"a\", encoding='utf-8') as log_file:\n",
    "    #     log_file.write(f\"{prompt}\\n\\n----------------------------\\n\\n\")\n",
    "\n",
    "    code_response = ai_client.generate_response(prompt, model= \"minimax/minimax-01\").strip()\n",
    "    matches = find_near_matches('Final Answer:', code_response, max_l_dist=2)\n",
    "    if not matches:  # Check if matches is empty\n",
    "        raise ValueError(\"Expected 'Final Answer:' in the response but not found.\")\n",
    "        \n",
    "    last_index = matches[-1].end  # Use the last match\n",
    "    code_response = code_response[last_index:].strip()\n",
    "    # Remove all markdown symbols\n",
    "    code_response = code_response.replace('**', '').replace('*', '').replace('`', '').replace('#', '').replace('_', '').strip().split()[0]\n",
    "\n",
    "    # Ensure the response is encoded in UTF-8 to avoid encoding errors\n",
    "    code_response = code_response.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "\n",
    "    return [{\n",
    "        \"code\": \"End-to-End model (no code visible)\", \n",
    "        \"result\": code_response,\n",
    "        \"success\": True\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_text(dataset_name: str, question: str, ai_client, is_sample: bool, truncate_limit: int = 10000000) -> str:\n",
    "    \"\"\"\n",
    "    Loads a dataset by name and converts the first 'truncate_limit' rows into a markdown representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dataset using the dataset_name\n",
    "    prompt = f\"Given the question: '{question}', which columns from the dataset are necessary to answer it? The dataset contains the following columns: {', '.join(load_table(dataset_name, is_sample).columns)}. Please provide extensive reasoning and only then the column names as a comma-separated list started with [ and ended with ].\"\n",
    "    # print(prompt)\n",
    "    columns_text = ai_client.generate_response(prompt).strip()\n",
    "    # print(f\"Columns identified: {columns_text}\")\n",
    "\n",
    "    columns_list = [col.strip().strip(\"'\\\"`\") for col in columns_text[columns_text.rfind('[')+1:columns_text.rfind(']')].split(',')]\n",
    "    # print(f\"Columns identified2: {columns_list}\")\n",
    "    \n",
    "    dataset = load_table(dataset_name, is_sample)[columns_list]\n",
    "    \n",
    "    \n",
    "    markdown_text = dataset.to_markdown()\n",
    "    # Ensure the markdown text is encoded in UTF-8 to avoid encoding errors\n",
    "    markdown_text = markdown_text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    # with open('dataset_markdown.txt', 'a', encoding='utf-8') as f:\n",
    "    #     f.write(markdown_text)\n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question reformulation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformulate_question(dataset_name: str, question: str, is_sample: bool, ai_client):\n",
    "    \"\"\"\n",
    "    Analyzes potential ambiguities in the question and reformulates it to be more precise.\n",
    "    Returns the reformulated question.\n",
    "    \"\"\"\n",
    "    # Get dataset info\n",
    "    df = load_table(dataset_name, is_sample)\n",
    "    schema = ', '.join([f\"{col} ({df[col].dtype})\" for col in df.columns])\n",
    "    sample_data = df.head(5).to_markdown()\n",
    "    sample_data = sample_data.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "\n",
    "    # Ask about ambiguities\n",
    "    ambiguities_prompt = f\"\"\"Given this dataset schema: {schema}\n",
    "And this sample data (attention: it is just 5 first rows, not whole dataset):\n",
    "{sample_data}\n",
    "\n",
    "For this question: \"{question}\"\n",
    "\n",
    "What are 3 potential severe ambiguities or unclear aspects that could prevent from answering this question correctly? Important: DO NOT PROVIDE YOUR OWN DEFINITIONS OR CONSTRAINTS, only use information from provided question and schema. ALWAYS CHECK IF RELEVANT COLUMN IS IN SCHEME AND INCLUDE IT INTO THE QUESTION. DONT ASK FOR UNIQUE WHERE IT IS NOT SPECIFIED EXPLICITLY.\"\"\"\n",
    "    \n",
    "    ambiguities = ai_client.generate_response(ambiguities_prompt, model=\"meta-llama/llama-3.3-70b-instruct\").strip()\n",
    "    \n",
    "    # Ask for reformulation\n",
    "    reformulation_prompt = f\"\"\"\"\n",
    "examples: \n",
    "Original request:\n",
    "Are there any Pokémon with a total stat greater than 700?\n",
    "Reformulated request:\n",
    "(\n",
    "\"details\": \"1. The original question lacks column specificity. We need to reference the exact column names.\n",
    "2. We should specify how Pokémon are uniquely identified in the dataset.\n",
    "3. Need to be precise about the total stat column name.\",\n",
    "\"result\": Are there any unique Pokémon, identified by their `number`, in the dataset where the `total` value is greater than 700?\n",
    ")\n",
    "\n",
    "Original request:\n",
    "How many posts are in Spanish?\n",
    "Reformulated request:\n",
    "(\n",
    "\"details\": \"1. The original question is vague about how Spanish language is identified.\n",
    "2. Need to specify the exact column name and value to check.\n",
    "3. Should clarify we want a count of matching posts.\",\n",
    "\"result\": What is the count of posts in the dataset where the 'lang' column value exactly matches 'es'?\n",
    ")\n",
    "\n",
    "Original request:\n",
    "What is the average rating of the posts in Russia from 2015 to 2025 in Moscow?\n",
    "Reformulated request:\n",
    "(\n",
    "\"details\": \"1. The original question specifies a geographic location and time frame, which needs to be reflected in the dataset.\n",
    "2. Need to clarify the exact column name for the rating.\n",
    "3. Should mention if we want the average rating for all posts or a specific subset based on the provided criteria.\",\n",
    "\"result\": What is the average rating of posts in the dataset where the 'status' column value is 'published' and the 'date' column is between '2015-01-01' and '2025-12-31' in Moscow?\n",
    ")\n",
    "\n",
    "#### Task Description:\n",
    "As the AI assistant, your task is to rewrite the NL entered by the user based on the given\n",
    "database information and reflection.\n",
    "This NL has some flaws and got bad generation in the downstream models, so you need to make this\n",
    "NL as reliable as possible.\n",
    "The rewritten NL should express more complete and accurate database information requirements\n",
    "as far as possible. In order to do this task well, you need to follow these steps to think and\n",
    "process step by step:\n",
    "1. Please review the given reflection and DB information, and first check whether the NL contains\n",
    "the corresponding key information and the corresponding flaws. If they exists, please modify,\n",
    "supplement or rewrite it in the statement of NL by combining the reflection and DB.\n",
    "2. Please rewrite the original NL based on the above process. On the premise of providing more\n",
    "complete and more accurate database information, the structure of the rewritten NL should be similar\n",
    "to the original statement as far as possible. All rewritten statements do not allow delimiters,\n",
    "clauses, additional hints or explanations. DONT CONVERT IT INTO QUERY. DONT ADD UNIQUE WHERE IT IS NOT SPECIFIED EXPLICITLY. PREFER NAMES INSTEAD OF IDs when presenting the answer.\n",
    "(\n",
    "\"details\": <YOUR STEP-BY-STEP THINKING DETAILS>,\n",
    "\"result\": <YOUR FINAL REWRITED NL>\n",
    ")\n",
    "### INPUT:\n",
    "SCHEMA: # Fill the database content\n",
    "{schema}\n",
    "NL: # Fill the flaw NL\n",
    "{question}\n",
    "Possible Ambiguities: \n",
    "{ambiguities}\n",
    "\n",
    "### OUTPUT:\n",
    "     \n",
    "    \"\"\"\n",
    "    try:\n",
    "        reformulated_question = ai_client.generate_response(reformulation_prompt, model=\"meta-llama/llama-3.3-70b-instruct\").strip()\n",
    "        reformulated_question = reformulated_question[reformulated_question.rfind('esult:') + len('esult:'):].strip()\n",
    "    except:\n",
    "        reformulated_question = question\n",
    "    return reformulated_question\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to incorporate all solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(dataset_name: str, question: str, is_sample: bool, ai_client):\n",
    "    \"\"\"\n",
    "    Main processing function that orchestrates the solution attempts.\n",
    "    \"\"\"\n",
    "    solutions = []\n",
    "\n",
    "    \n",
    "\n",
    "    try:\n",
    "        solutions = get_python_solutions(question, dataset_name, is_sample, ai_client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_python_solutions: {e}\")\n",
    "        solutions = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    failed_solutions = [s for s in solutions if not s[\"success\"]]\n",
    "\n",
    "    sql_results = []\n",
    "    try:\n",
    "        sql_results = sql_fallback(dataset_name, question, is_sample, failed_solutions, ai_client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sql_fallback: {e}\")\n",
    "\n",
    "    print(f\"Time for sql_fallback: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    if sql_results:\n",
    "        solutions.extend(sql_results)\n",
    "\n",
    "    e2e_results = []\n",
    "    try:\n",
    "        dataset_text = dataset_to_text(dataset_name, question, ai_client, is_sample) \n",
    "    except Exception as e:\n",
    "        print(f\"Error during dataset_to_text: {e}\")\n",
    "    try:\n",
    "        e2e_results = e2e_response(question, dataset_text, ai_client)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during e2e_results: {e}\")\n",
    "\n",
    "    if len(e2e_results) > 0:\n",
    "        solutions.extend(e2e_results)\n",
    "\n",
    "\n",
    "    # print(\"All solutions:\", solutions)\n",
    "\n",
    "    successful_solutions = [s for s in solutions if s[\"success\"]]\n",
    "    # print(\"Successful:\", successful_solutions)\n",
    "    if len(successful_solutions) > 0:\n",
    "        start_time = time.time()\n",
    "        with open(\"2_test.txt\", \"a\", encoding='utf-8') as f:\n",
    "            f.write(f\"---\\n{question}\\n{solutions}\")\n",
    "        final_result = vote_on_solutions(successful_solutions, question, dataset_name, is_sample, ai_client)\n",
    "        print(f\"Time for vote_on_solutions: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        if isinstance(final_result, str):\n",
    "            final_result = final_result.replace('\"', '').replace(\"'\", '')\n",
    "        elif isinstance(final_result, list):\n",
    "            final_result = str(final_result[0] if len(final_result) == 1 else final_result)\n",
    "\n",
    "        return final_result, solutions\n",
    "    return \"No solution\", []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def main():\n",
    "    batch_size = 1\n",
    "    # Check how many lines are already in the file\n",
    "    if os.path.exists(\"solution_results.txt\"):\n",
    "        with open(\"solution_results.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            processed_count = sum(1 for _ in f)\n",
    "    else:\n",
    "        processed_count = 0\n",
    "    print(f\"Starting to fill the file from line N {processed_count+1}\")\n",
    "    ai_client=OpenAIClient()\n",
    "    dataset = load_data()\n",
    "    start_index = processed_count\n",
    "    end_index = min(start_index + batch_size, len(dataset))\n",
    "    dataset = dataset.select(range(start_index, end_index))\n",
    "    \n",
    "\n",
    "    responses = []\n",
    "    # Process the current batch and append responses one by one\n",
    "    with open(\"solution_results.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        for row in tqdm(dataset):\n",
    "            try:\n",
    "                response, log_info = process_response(row[\"dataset\"], row[\"question\"], is_sample=False, ai_client=ai_client)\n",
    "                f.write(f\"{str(response).replace(chr(10), ' ')}\\n\")\n",
    "                responses.append(response)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred on row {row}: {e}\")\n",
    "                break\n",
    "\n",
    "    accuracy, *meta_info = evaluate_responses(responses, dataset, is_sample=False)\n",
    "    print(f\"DataBench accuracy: {accuracy}\")\n",
    "\n",
    "    with open(\"solution_results_logs.txt\", \"a\", encoding=\"utf-8\") as log_file:\n",
    "        for sublist in meta_info:\n",
    "            for response, truth, semantic, question, verdict in sublist:\n",
    "                log_file.write(f\"{response=} | {truth=} | {semantic=} | {question=} | {accuracy=} | {verdict=}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in range(100):\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
