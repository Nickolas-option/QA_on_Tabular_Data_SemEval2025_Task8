# Team Anotheroption at SemEval-2025 Task 8
Hybrid Table QA with RAG, Text-to-Code, and LLM-Guided Orchestration

This repository contains the code and related materials for our submission to SemEval-2025 Task 8: Question Answering (QA) over tabular data. Our team, "Anotheroption," developed a hybrid system to address this challenging task using the DataBench benchmark.

## Project Overview

This project focuses on building a system capable of accurately answering natural language questions posed against data presented in tabular format The core of our approach involves a combination of techniques including:

*   **RAG (Retrieval-Augmented Generation):**  We use RAG to enhance the prompts of our models for question answering.
*   **Text-to-Code Generation (SQL and Pandas):** We translate natural language questions into executable SQL and Pandas code to query the data tables.
*   **Self-Correction:**  A mechanism to identify and correct errors in code generation.
*   **End-to-End (E2E) Answer Generation:** A method to directly generate answers using text-based representations of the tables.
*   **LLM-Guided Orchestration:** An LLM to select the most probable solution from multiple candidate answers generated by the other components.

## Key Components & Technologies

*   **LLMs (Large Language Models):** We utilized several state-of-the-art instruction-tuned models, including Llama 3, Codestral, Qwen Coder Instruct, and MiniMax-01.
*   **Programming Languages:** Python is extensively used to build the overall pipeline.

## Repository Structure
 * pipeline.ipynb file
 * requitements.txt
 * solution scheme

## Installation

1.  **Clone the repository:**

    ```bash
    git clone https://github.com/Nickolas-option/QA_on_Tabular_Data_SemEval2025_Task8
    cd QA_on_Tabular_Data_SemEval2025_Task8
    ```

2.  **Create a virtual environment (recommended):**

    ```bash
    python -m venv .venv
    source .venv/bin/activate  # Linux/macOS
    # .\.venv\Scripts\activate  # Windows
    ```

3.  **Install dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

## Running the Code

1.  **Run the pipeline_show_version.ipynb notebook:**
    *   Open the `pipeline_show_version.ipynb` notebook in your preferred environment (e.g., Jupyter Notebook, Google Colab).
    *   **[Important:]** You may need to set up corresponding APIs for your LLMs for the pipeline to work correctly.
    *   Adjust parameters or inputs within the notebook to experiment.

## Data

*   The primary dataset used and described for the models used is Databench.
*   The notebook downloads the needed data automatically.

## Competition Results

*   We ranked in the top 13 out of 38 teams in the competitionâ€™s OpenSource-models-only section, achieving an accuracy score of 80 on the Databench evaluation.
*   In the global ranking, which includes proprietary models, we placed in the top 20 out of 53 teams while exclusively using open-source models.



